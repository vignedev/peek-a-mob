import os
import argparse
from os import path
from utils import annotate_file, get_entity_bidict
import cv2 as cv
from multiprocessing import Pool
import random
import sys

RANDOM_TRAIN_RATIO=0.7
RANDOM_VALID_RATIO=0.2
RANDOM_TEST_RATIO=0.1 # ignored btw

def get_argv():
  parser = argparse.ArgumentParser()
  parser.add_argument(
    '-i', '--input',
    help='input folder where the quadrant-screenshots are located at',
    required=True
  )
  parser.add_argument(
    '-r', '--recursive',
    help='pull all files in the folder and subfolders',
    required=False,
    action='store_true'
  )
  parser.add_argument(
    '-e', '--entities',
    help='location of entitites.json generated by the `minecraft_shader/generate_entittyprop.js`',
    required=True
  )
  parser.add_argument(
    '-o', '--output',
    help='output folder, where the images and their annotations are going to be',
    required=True
  )
  parser.add_argument(
    '-t', '--type',
    help='should this be a train, validation or test dataset. or sort/random_sort, which will spread them around in 70:20:10 ratio. random will randomize the order!',
    default='train',
    choices=['train', 'valid', 'test', 'sort', 'random_sort']
  )
  parser.add_argument(
    '-f', '--format',
    help='which format to output the annotations as, or rather what should the first two normalized coordinates represent\n'
         '(center: center of bounding box, bbox: top-left corner of bounding box)',
    choices=['center', 'bbox'],
    required=True
  )
  parser.add_argument(
    '-n', '--ncpu',
    help='how many threads to use for the conversion',
    type=int
  )
  parser.add_argument(
    '-d', '--debug',
    help='draws the bounding boxes onto cropped images (do not use for generating!!)',
    action='store_true'
  )
  parser.add_argument(
    '-x', '--extension',
    help='the file extension for the final images to use (eg. for images.png use "png")',
    default='png'
  )
  parser.add_argument(
    '-a', '--area_threshold',
    help='normalized area threshold, values below this would get filtered out (set to 0.0 to disable)',
    default=0.0,
    type=float
  )
  return parser.parse_args()

def create_from_image(filepath: str, _type: str, argv: argparse.Namespace):
  name = '.'.join(path.basename(filepath).split('.')[:-1])
  image, labels = annotate_file(filepath, format=argv.format, debug_draw=argv.debug, area_threshold=argv.area_threshold)

  cv.imwrite(path.join(argv.output, _type, 'images', f'{name}.{argv.extension}'), image)
  with open(path.join(argv.output, _type, 'labels', f'{name}.txt'), 'w') as hfile:
    hfile.write('\n'.join([ ' '.join([ str(w) for w in v ]) for v in labels ]))

  found_entities = set([ int(t[0]) for t in labels ])
  return filepath, found_entities

def create_from_image_tuple(data: tuple[str, str, argparse.Namespace]):
  filepath, _type, argv = data
  return create_from_image(filepath, _type, argv)

def get_dataset_images(root: str, recursive: bool = False):
  """
  Make sure it returns the JOINED PATH with root
  """
  if not recursive:
    return [ path.join(root, file) for file in os.listdir(root) if os.path.isfile(path.join(root, file)) ]
  else:
    bucket = []
    for rootdir, subdir, files in os.walk(root):
      segments = rootdir.split(os.sep)
      has_dot = False
      for seg in segments:
        if seg.startswith('.') and not (seg == '.' or seg == '..'):
          has_dot = True
          break

      if has_dot:
        continue

      for file in files:
        bucket.append(path.join(rootdir, file))
    return bucket

def get_label_files(root: str):
  bucket = []
  for rootdir, subdir, files in os.walk(root):
    segments = rootdir.split(os.sep)

    if segments[-1] != 'labels':
      continue

    for file in files:
      if file.endswith('.txt'):
        bucket.append(path.join(rootdir, file))
  return bucket

def remap_label(path: str, remapper: dict[int, int]):
  buckets = []
  with open(path, 'rt') as file:
    for line in file:
      segments = line.split(' ')
      segments[0] = str(remapper[int(segments[0])])
      buckets.append(' '.join(segments))
  with open(path, 'wt') as file:
    file.write(''.join(buckets))

  return len(buckets)

def remap_label_tuple(data: tuple[str, dict[int, int]]):
  path, remapper = data
  return remap_label(path, remapper)

if __name__ == '__main__':
  argv = get_argv()
  entity_bidict, entity_json = get_entity_bidict(argv.entities)

  # list of files
  files: list[tuple[str, str]] = [ (file, argv.type) for file in get_dataset_images(argv.input, argv.recursive) ]
  n_files = len(files)

  # sorter
  if argv.type in ['sort', 'random_sort']:
    if argv.type == 'random_sort':
      random.shuffle(files)

    for i in range(n_files):
      _file, _type = files[i]
      ratio = (i+1) / n_files
      if ratio <= RANDOM_TRAIN_RATIO:
        files[i] = (_file, 'train')
      elif ratio <= (RANDOM_TRAIN_RATIO + RANDOM_VALID_RATIO):
        files[i] = (_file, 'valid')
      else:
        files[i] = (_file, 'test')

  # ensure that output dir structure exists
  os.makedirs(argv.output, exist_ok=True)
  for folder in ['train', 'valid', 'test']:
    os.makedirs(path.join(argv.output, folder, 'images'), exist_ok=True)
    os.makedirs(path.join(argv.output, folder, 'labels'), exist_ok=True)

  # lets get the show on the f-cking road
  counter = 0
  pool = Pool(processes=argv.ncpu)
  unique_entities = set()
  for name, found_entities in pool.imap_unordered( create_from_image_tuple, [ (_file, _type, argv) for _file, _type in files ]):
    counter += 1
    for ent in found_entities:
      unique_entities.add(ent)

    if counter == n_files or counter % 100 == 0:
      sys.stdout.write(f'\r[i] finished ({counter}/{n_files}) {name}')

  print('[i] dataset creation is done, performing remapping IDs')

  # create conversion remapping
  remapper = dict()
  for src, dst in zip(unique_entities, range(len(unique_entities))):
    remapper[src] = dst

  # remap the existing labels
  label_files = get_label_files(argv.output)
  n_files = len(label_files)
  counter = 0
  for cnt in pool.imap_unordered( remap_label_tuple, [ (path, remapper) for path in label_files ] ):
    counter += 1
    if counter == n_files or counter % 100 == 0:
      sys.stdout.write(f'\r[i] remapped {cnt} ({counter}/{n_files})')

  # write the intro yaml (if not existing)
  data_path = path.join(argv.output, 'data.yaml')
  if not path.exists(data_path) or True: # doesnt hurt much, right?
    with open(data_path, 'w') as file:
      names = ', '.join([ f'\'{ent}\'' for ent in entity_json ])
      file.write('\n'.join([
        f'train: train/images',
        f'val: valid/images',
        f'test: test/images',
        f'',
        f'names:',
        f''
        # f'nc: {len(entity_json)}',
        # f'names: [{names}]'
      ]))

      file.write('\n'.join([ f'    {remapper[id]}: {entity_bidict[id]}' for id in unique_entities ]))
  
  print('[i] finito')